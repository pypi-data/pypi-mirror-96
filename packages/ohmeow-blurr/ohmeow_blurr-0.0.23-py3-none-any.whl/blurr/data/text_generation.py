# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01e_data-text-generation.ipynb (unless otherwise specified).

__all__ = ['HF_TextGenerationInput', 'HF_TextGenerationBatchTransform']

# Cell
import ast
from functools import reduce

import torch
from transformers import *
from fastai2.text.all import *

from ..utils import *
from .core import *

# Cell
class HF_TextGenerationInput(list): pass

# Cell
class HF_TextGenerationBatchTransform(HF_BatchTransform):
    def __init__(self, hf_arch, hf_tokenizer, **kwargs):
        super().__init__(hf_arch, hf_tokenizer, HF_TextGenerationInput, **kwargs)

    def encodes(self, samples):
        samples = super().encodes(samples)
        if (len(samples[0]) == 1): return samples

        updated_samples = []
        for s in samples:
            s[0]['decoder_input_ids'] = s[1]['input_ids'][:-1].clone()
            s[0]['labels'] = s[1]['input_ids'][1:].clone()
            s[0]['labels'][s[0]['labels'] == self.hf_tokenizer.pad_token_id] = -100

            targ_ids = s[1]['input_ids']

            updated_samples.append((s[0], targ_ids))

        return updated_samples

    def decodes(self, encoded_samples):
        if (isinstance(encoded_samples, dict)): return self.hf_input_return_type([encoded_samples['input_ids']])
        return [encoded_samples]

# Cell
@typedispatch
def show_batch(x:HF_TextGenerationInput, y, samples, dataloaders=None, ctxs=None, max_n=6, **kwargs):
    res = L([ (s[0], s[1]) for s in samples ])
    display_df(pd.DataFrame(res, columns=['text', 'target'])[:max_n])
    return ctxs