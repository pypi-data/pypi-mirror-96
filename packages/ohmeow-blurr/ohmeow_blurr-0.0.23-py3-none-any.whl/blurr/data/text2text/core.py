# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01za_data-text2text-core.ipynb (unless otherwise specified).

__all__ = ['HF_Text2TextAfterBatchTransform', 'HF_Text2TextBlock']

# Cell
from functools import reduce

import torch
from transformers import *
from fastai.text.all import *

from ...utils import *
from ..core import *

logging.set_verbosity_error()

# Cell
class HF_Text2TextAfterBatchTransform(HF_AfterBatchTransform):
    def decodes(self, encoded_samples):
        input_ids = encoded_samples['input_ids'] if (isinstance(encoded_samples, dict)) else encoded_samples
        return self.input_return_type(input_ids, hf_tokenizer=self.hf_tokenizer)


class HF_Text2TextBlock(HF_TextBlock):

    def __init__(self, hf_arch=None, hf_tokenizer=None, before_batch_tfms=None, after_batch_tfms=None,
                 max_length=None, padding=True, truncation=True, is_split_into_words=False,
                 n_tok_inps=1, tok_kwargs={}, input_return_type=HF_BaseInput, dl_type=SortedDL,
                 before_batch_kwargs={}, after_batch_kwargs={}, **kwargs):

        if (after_batch_tfms is None):
            after_batch_tfms = HF_Text2TextAfterBatchTransform(hf_tokenizer, input_return_type,
                                                               **after_batch_kwargs.copy())

        return super().__init__(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer,
                                before_batch_tfms=before_batch_tfms, after_batch_tfms=after_batch_tfms,
                                max_length=max_length, padding=padding, truncation=truncation,
                                is_split_into_words=is_split_into_words, n_tok_inps=n_tok_inps,
                                tok_kwargs=tok_kwargs, input_return_type=input_return_type, dl_type=dl_type,
                                before_batch_kwargs=before_batch_kwargs, after_batch_kwargs=after_batch_kwargs,
                                **kwargs)